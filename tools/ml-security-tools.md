## Recommended tools for securing machine learning systems

| Category | Tool |
| -------- | ---- |
| ML source code quality & security | https://codeql.github.com/ |
| Third-party library security vulnerability scan | https://pypi.org/project/safety/ |
| Secure Jupyter Notebooks | https://nbdefense.ai/ |
| Benchmark ML vulnerabilities | https://github.com/cleverhans-lab/cleverhans |
| Attack & Defense | https://github.com/Trusted-AI/adversarial-robustness-toolbox |
| Attack & Defense | https://github.com/BorealisAI/advertorch |
| Attack & Defense | https://github.com/bethgelab/foolbox |
| Attack & Defense (on NLP) | https://github.com/QData/TextAttack |
| Attack & Defense (Fuzz) | https://github.com/mindspore-ai/mindarmour |
| ML Model Vulnerability Scanning | https://github.com/twosixlabs/armory |
| LLM vulnerability scanner | https://github.com/leondz/garak/ |
| Assess the Robustness of LLM endpoints | https://github.com/Azure/PyRIT |
| Scans models to determine if they contain unsafe code | https://github.com/protectai/modelscan |
| Evaluate Security of AI model | https://github.com/Azure/counterfit |
| Privacy & Compliance | https://github.com/ibm/ai-privacy-toolkit |

*Disclaimer: Thales does not endorse the utilization of any specific tool. The reference to such tools is purely for illustrative and informational purposes, and their use should be exercised with discretion.*